These are the following assumptions for BYOM.

What we know from the data is that they are tweets.

- Assumption 1: case-sensitive models are not needed.
Why?: Since they are tweet, we know by experience that most people do not tweet using proper grammar in regards to case sensitivity.
We believe that extra columns in our n-gram only adds overhead compared to the benefit they bring in terms of accuracy.
We will confirm on infirm this assumption for all languages based on our models.

- Assumption 2: Out of range character (Outside vocabulary V) will mostly be emoticons and grammar signs (like "",:; etc...) for V=2.
Why>: Since we are dealing with tweets where most of the time people tweeting are expressing their opinions, most of those tweets are written in a way to be
understood by people reading them. This is why out of range characters (characters out of the vocabulary V) are mostly emoticons because rarely people



FOR BYOM
- Assumption: Have a dedicated vocabulary for each languages that considers both the alphabet of that language (with signs), and the punctation marks (;,;'" ...).
We think some languages will have more recurring characters given certain type of punctuation marks and would contribute greatly to the overall classifier.

Look at which additive smoothing value the model performs the best.



In unigram, one possible explanation of why it performs poorly is that,
even though some languages may have character frequency different from other laguages (e is more present in spanish than german for example),
it may perform poorly because the test set of training data does not contain characters that emphasizes this gap the would be found if more training data was available
or if the test set had a better representation of the languages feature


Say that in most languages, the character frequency is more or less the same (google up stats for that.) therefore, since the character frequency is more or less the same,
they dont tell us much about the feature of ech languages, this is why its not reliable and classification is bad.


Generate classifiers with many different data  for delta 0, 0.1,0.2,0.3 ... until 1. Explain why you chose those value and which one give the most accurate results


Also, see if instead of using vocab 0,1,2 For each languages, have its own vocab with all letters and punctuation marks

Many words in the tweet are links and unsername which contains to vocabulary significance and can recude presicision when training. Those samples are not good representation of the languages


Talk about how accurany changes when u change only the vocabulary, when u change only delta, when u change only ngramm.

Say which languages were mostly failed correct as a graph or table
es :75% right 25% wrong
pt 40% right 60%
. . .
out of all the wrong/correct, which ones  came from which language


in the testing phase, there is only in true case of gl. in some trace file, we have many guesses that gl is the right answer and end up wrong (look at that)